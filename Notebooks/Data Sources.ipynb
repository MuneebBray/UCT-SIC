{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Sources.ipynb","provenance":[],"collapsed_sections":["LRTDOjJfcvNJ","ErULNVtViVZI","SfcA9sgUq5X0","gMzckDO-4prs","0IBNvazrxpsV"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1S7P82Aq8vkU","cellView":"form","executionInfo":{"status":"ok","timestamp":1603995035378,"user_tz":-120,"elapsed":3245,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"c8d8701e-39b0-436b-b722-b08ff0332f49","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title # Run to Setup\n","import os\n","import cv2\n","import requests\n","import ftplib\n","#import random\n","\n","import datetime as dt\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import geopy.distance as geoDist\n","import pandas as pd\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from IPython.display import clear_output\n","\n","try:\n","  import sentinelsat\n","except:\n","  !pip install sentinelsat\n","  import sentinelsat\n","  \n","try:\n","  import netCDF4 as nc\n","except:\n","  !pip install netCDF4\n","  import netCDF4 as nc\n","\n","# Mount drive\n","clear_output()\n","print(\"Mount Google Drive\")\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Global Variables\n","image_dimension = 128\n","\n","# OData Authentication\n","# Enter your credentials here\n","odata_username = \"\"\n","odata_password = \"\"\n","search_api = sentinelsat.SentinelAPI(odata_username, odata_password)\n","\n","# CMEMS Authentication\n","# Enter your credentials here\n","cmems_username = \"\"\n","cmems_password = \"\"\n","\n","# Dataset folder names\n","dataset_location_path   = \"/content/gdrive/My Drive/AKM05/DataSets\"\n","#/content/drive/My Drive/location_of_the_file\n","\n","#SENTINEL-1 Quicklook File Save Name\n","sentinel_1_folder         = \"SENTINEL-1 SAR\"\n","sentinel_1_quicklook_name = \"Quick Look Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","sentinel_1_resampled_name = \"Resampled Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","\n","# CMEMS File Save Name\n","cmems_label_folder = \"CMEMS Concentration\"\n","cmems_name         = \"NC Files/{0}/{1}/{0}_{1}_{2}.nc\" # 0:YEAR, 1:MONTH, 2:DAY\n","concentration_name = \"Concentration Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","uncertainty_name   = \"Uncertainty Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","\n","# Validation\n","validations_folder    = \"Image Processing Validation\"\n","interpolation_name    = \"Interpolation Validation/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","label_comparison_name = \"Patch Label Validation/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n","\n","# Batch size for training and testing\n","batch_size = 16\n","seed = 1\n","\n","# Show the available datasets\n","clear_output()\n","dataset_location_path   = \"/content/gdrive/My Drive/AKM05/DataSets\"\n","\n","print(\"The following datasets are available:\")\n","for name in os.listdir(dataset_location_path):\n","  print(\"\\t\" + name)\n","print(\"Please make sure that the new dataset name is unique.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following datasets are available:\n","\tSouthern_Freezing_3_20190901_20191130\n","\tSouthern_Melting_3_20200101_20200331\n","\tNorthern_Freezing_6_20200101_20200630\n","\tSouthern_Freezing_6_20200401_20200930\n","\tData Information.gdoc\n","\tNorthern_Melting_3_20200801_20201029\n","\tSouthern_Melting_6_20200101_20200630\n","Please make sure that the new dataset name is unique.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pBd3G8NI01Iv","cellView":"form","executionInfo":{"status":"ok","timestamp":1603995045550,"user_tz":-120,"elapsed":2891,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"0e79f7f7-0cdc-46c1-eb30-4061a356300d","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title # Step 1: Dataset Parameters\n","#@markdown ---\n","#@markdown ###Choose a name for this dataset\n","dataset_name = \"Southern_Melting_6\" #@param {type:\"string\"}\n","#@markdown ---\n","#@markdown ###Select the start date (included)\n","start_date = \"2020-01-01\" #@param {type:\"date\"}\n","#@markdown ###Select the end date (excluded)\n","end_date   = \"2020-06-30\" #@param {type:\"date\"}\n","#@markdown ---\n","#@markdown ###Choose region of interest\n","ROI = \"Antarctic Circle\" #@param [\"Arctic Circle\", \"Antarctic Circle\", \"South Sandwich Islands\", \"Other North\", \"Other South\"]\n","#@markdown ---\n","#------------------------------------------------------------------------------#\n","\n","# Dataset Information\n","start_date   = start_date.replace(\"-\", \"\")\n","end_date     = end_date.replace(\"-\", \"\")\n","dataset_name = \"{}_{}_{}\".format(dataset_name, start_date, end_date)\n","\n","os.chdir(dataset_location_path)\n","if dataset_name not in os.listdir():\n","  # Make the dataset folder\n","  os.mkdir(dataset_name)\n","\n","  # Make the folders for label comparisons\n","  # os.mkdir(os.path.join(dataset_name, validations_folder))\n","  # os.mkdir(os.path.join(dataset_name, validations_folder, \"Interpolation Validation\"))\n","  # os.mkdir(os.path.join(dataset_name, validations_folder, \"Patch Label Validation\"))\n","  \n","  # Done making new folders.\n","  print(\"Made new directory.\")\n","else:\n","  print(\"Directory already exists.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Directory already exists.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PRK6-M9FTktG"},"source":["# Step 2: SENTINEL-1 Data Acquisition"]},{"cell_type":"markdown","metadata":{"id":"qgwALtOhU_cE"},"source":["**Search**\n","\n","Query the Copernicus database using the Sentinelsat API (which uses OpenSearch)\n","\n","This produces a dictionary of all search results."]},{"cell_type":"code","metadata":{"id":"RdTgt_x_0y37","cellView":"form"},"source":["#@title Find SENTINEL-1 Data\n","#13\n","\n","#Create directory for SENTINEL data\n","os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if sentinel_1_folder not in os.listdir(): os.mkdir(sentinel_1_folder)\n","\n","# Define ROI\n","# Useful site for generating and checking these polygons: https://arthur-e.github.io/Wicket/sandbox-gmaps3.html\n","roi_polygons = {\"Arctic Circle\"           : \"POLYGON((-180 66,-180 85,0 85,180 85,180 66,0 66,-180 66))\",\n","                \"Antarctic Circle\"        : \"POLYGON((-180 -66,-180 -85,0 -85,180 -85,180 -66,0 -66,-180 -66))\",\n","                \"South Sandwich Islands\"  : \"POLYGON((-29.30712890625 -55.480494204910514,-25.966796875 -55.975418279377394,-25.36279296875 -59.589097877324384,-28 -60,-29.30712890625 -55.480494204910514))\",\n","                \"Other North\"             : \"<Enter a Northern hemisphere polygon here if you want to define some other region>\",\n","                \"Other South\"             : \"<Enter a Southern hemisphere polygon here if you want to define some other region>\"}\n","\n","# Define additional keyword search items\n","kwargs = {'platformname':'Sentinel-1',\n","          'producttype':'GRD',\n","          'sensoroperationalmode':'EW',\n","          'polarisationmode':'HH+HV'\n","         }\n","\n","# Query for matching products\n","while (True):\n","  try:\n","    # This often times out, so try is as many times as is necessary, unless the user cancels\n","    products = search_api.query(area=roi_polygons[ROI],\n","                                date=(start_date, end_date),\n","                                # date=(\"20190101\", \"20190301\"),\n","                                limit=None,\n","                                offset=0,\n","                                **kwargs)\n","    break\n","  except KeyboardInterrupt:\n","    break\n","  except:\n","    print(\"Timed out - trying again.\")\n","\n","print(\"\\nThere are {} results matching the search criteria.\".format(len(products)))\n","\n","# Write all search results uuids to txt file\n","os.chdir(sentinel_1_folder)\n","with open(\"Search Results.txt\", 'w') as f:\n","  for uuid in products.keys():\n","    f.write(uuid + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmgEaombVve7"},"source":["**Download**\n","\n","Save all quicklook files to Google Drive (retry failed downloads)"]},{"cell_type":"code","metadata":{"id":"hj1yWuH1V8-z","cellView":"form"},"source":["#@title Download SENTINEL-1 Data\n","# Root access path\n","# service_root_URI = \"https://scihub.copernicus.eu/dhus/odata/v1/\"\n","service_root_URI = \"https://scihub.copernicus.eu/apihub/odata/v1/\"\n","\n","# Move into the SENTINEL-1 folder\n","os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","\n","# Remove the failed downlaods file so that those items can be retried\n","if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n","\n","# Lists to track downloads\n","successful_downloads = []\n","needs_downloading = {}\n","\n","# Do not want to re-download files already on the drive, so remove them from the list\n","if \"Successful_Downloads.txt\" in os.listdir():\n","  with open(\"Successful_Downloads.txt\", 'r') as f:\n","    successful_downloads = [item.split(\"_\")[-1] for item in f.read().splitlines()] # Extract the UUID from the file name\n","\n","# All failed downloads will be retried automatically if they are contained in 'products'\n","for uuid in products:\n","  if uuid not in successful_downloads:\n","    needs_downloading[uuid] = products[uuid]\n","\n","# Download all items which haven't already been downloaded\n","for uuid, value in needs_downloading.items():\n","  # clear_output()\n","  print(\"Downloading product {} of {}: {}\\n\".format(list(needs_downloading.keys()).index(uuid) + 1, len(needs_downloading), uuid))\n","  # quicklook_url = os.path.join(service_root_URI, \"Products('{}')/Products('Quicklook')/$value\".format(uuid))\n","  quicklook_url = value['link_icon']\n","  print(quicklook_url)\n","  response = requests.get(quicklook_url, auth=(odata_username, odata_password))\n","  if (response.status_code == 200):\n","    date = value['endposition']\n","    if not os.path.exists(\"Quick Look Images/{}/{}/{}\".format(date.year, date.month, date.day)): os.makedirs(\"Quick Look Images/{}/{}/{}\".format(date.year, date.month, date.day))\n","    if not os.path.exists(\"Resampled Images/{}/{}/{}\".format(date.year, date.month, date.day)): os.makedirs(\"Resampled Images/{}/{}/{}\".format(date.year, date.month, date.day))\n","    try:\n","      # Save the quicklook image\n","      open(sentinel_1_quicklook_name.format(date.year, date.month, date.day, uuid), 'wb').write(response.content)\n","      original_image = cv2.imread(sentinel_1_quicklook_name.format(date.year, date.month, date.day, uuid))\n","\n","      # Save a resampled version of the image\n","      cv2.imwrite(sentinel_1_resampled_name.format(date.year, date.month, date.day, uuid), cv2.resize(original_image, (image_dimension, image_dimension)))\n","\n","      # Add this uuid to the list of successful downloads\n","      with open(\"Successful_Downloads.txt\", 'a+') as f:\n","        f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n","    except:\n","      # Failed when trying to write save image files\n","      with open(\"Failed_Downloads.txt\", 'a+') as f:\n","        f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n","  else:\n","    # Bad response from server\n","    print(\"Bad response from server\")\n","    with open(\"Failed_Downloads.txt\", 'a+') as f:\n","      f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n","\n","# Sumarise\n","print(\"Done saving search data.\")\n","if \"Failed_Downloads.txt\" in os.listdir():\n","  with open(\"Failed_Downloads.txt\", 'r') as f:\n","    failed_downloads = f.read().splitlines()\n","else:\n","  failed_downloads = []\n","print(\"Tried to save {} images, {} failed.\".format(len(needs_downloading), len(failed_downloads)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRTDOjJfcvNJ"},"source":["## Delete all SENTINEL-1 data - Be careful!"]},{"cell_type":"code","metadata":{"id":"LWK5URrWIKsE"},"source":["os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if sentinel_1_folder in os.listdir():\n","  os.chdir(sentinel_1_folder)\n","\n","  if \"Search Results.txt\" in os.listdir(): os.remove(\"Search Results.txt\")\n","  if \"Successful_Downloads.txt\" in os.listdir(): os.remove(\"Successful_Downloads.txt\")\n","  if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n","\n","  if \"Quick Look Images\" in os.listdir():\n","    for y in os.listdir(\"Quick Look Images\"):\n","      for m in os.listdir(os.path.join(\"Quick Look Images\", y)):\n","        for d in os.listdir(os.path.join(\"Quick Look Images\", y, m)):\n","          for f in os.listdir(os.path.join(\"Quick Look Images\", y, m, d)):\n","            os.remove(os.path.join(\"Quick Look Images\", y, m, d, f))\n","          os.rmdir(os.path.join(\"Quick Look Images\", y, m, d))\n","        os.rmdir(os.path.join(\"Quick Look Images\", y, m))\n","      os.rmdir(os.path.join(\"Quick Look Images\", y))\n","    os.rmdir(\"Quick Look Images\")\n","\n","  if \"Resampled Images\" in os.listdir():\n","    for y in os.listdir(\"Resampled Images\"):\n","      for m in os.listdir(os.path.join(\"Resampled Images\", y)):\n","        for d in os.listdir(os.path.join(\"Resampled Images\", y, m)):\n","          for f in os.listdir(os.path.join(\"Resampled Images\", y, m, d)):\n","            os.remove(os.path.join(\"Resampled Images\", y, m, d, f))\n","          os.rmdir(os.path.join(\"Resampled Images\", y, m, d))\n","        os.rmdir(os.path.join(\"Resampled Images\", y, m))\n","      os.rmdir(os.path.join(\"Resampled Images\", y))\n","    os.rmdir(\"Resampled Images\")\n","\n","  if len(os.listdir()) == 0:\n","    os.chdir(\"..\")\n","    os.rmdir(sentinel_1_folder)\n","\n","  print(\"All files should have been deleted.\")\n","else:\n","  print(\"No SENTINEL-1 data to delete.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTI24fiVTwqr"},"source":["# Step 3: CMEMS Data Acquisition"]},{"cell_type":"markdown","metadata":{"id":"b4BgR_4iZxpN"},"source":["**Download**\n","\n","Connect to the FTP server, and save the required .nc files to Google Drive."]},{"cell_type":"code","metadata":{"id":"TFQbLskNZwm_","cellView":"form","executionInfo":{"status":"ok","timestamp":1603999274360,"user_tz":-120,"elapsed":559420,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"ca681fc4-b2c0-42b2-ea8d-26bb7dbbbdd0","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title Download CMEMS Data\n","# Create directory for CMEMS data\n","os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if cmems_label_folder not in os.listdir(): os.mkdir(cmems_label_folder)\n","os.chdir(cmems_label_folder)\n","\n","# These ROIs will be treated as northern locations, all others will be assumed to be in the South\n","north_keys = [\"Arctic Circle\", \"Other North\"]\n","\n","# Product information (Adjust directory names based on which hemisphere has been selected in the ROI)\n","server_name    = \"nrt.cmems-du.eu\"\n","product_family = \"SEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_001\"\n","product_name   = \"METNO-GLO-SEAICE_CONC-{}-L4-NRT-OBS\".format(\"NORTH\" if ROI in north_keys else \"SOUTH\")\n","file_on_server = \"{0}/{1}/ice_conc_{3}h_polstere-100_multi_{0}{1}{2}1200.nc\".format(\"{0:04d}\", \"{1:02d}\", \"{2:02d}\", \"n\" if ROI in north_keys else \"s\")\n","\n","# Start date\n","start_datetime = dt.datetime(int(start_date[:4]), int(start_date[4:6]), int(start_date[6:]))\n","\n","# End date (inclusive)\n","end_datetime   = dt.datetime(int(end_date[:4]), int(end_date[4:6]), int(end_date[6:]))\n","\n","# Lists to track downloads\n","successful_downloads = []\n","failed_downloads = []\n","\n","# Don't want to repeat already downloaded products\n","if \"Successful_Downloads.txt\" in os.listdir():\n","  with open(\"Successful_Downloads.txt\", 'r') as file:\n","    successful_downloads = file.read().splitlines()\n","\n","# Connect to the CMEMS FTP server\n","server = ftplib.FTP()\n","server.connect(server_name)\n","server.login(cmems_username, cmems_password)\n","\n","# Locate the desired product\n","server.cwd(\"Core\")\n","server.cwd(product_family)\n","server.cwd(product_name)\n","\n","# Iterate through the desired files\n","date = start_datetime\n","download_count = 0\n","while date < end_datetime:\n","  if \"{}_{}_{}\".format(date.year, date.month, date.day) not in successful_downloads:\n","    # File has not already been downloaded, so download it now\n","    # clear_output()\n","    print(\"Downloading file number {0}: {1:04d}-{2:02d}-{3:02d}\".format(download_count + 1, date.year, date.month, date.day))\n","    if not os.path.exists(\"NC Files/{}/{}\".format(date.year, date.month)): os.makedirs(\"NC Files/{}/{}\".format(date.year, date.month))\n","    try:\n","      server.retrbinary(\"RETR \" + file_on_server.format(date.year, date.month, date.day), open(cmems_name.format(date.year, date.month, date.day), 'wb').write)\n","      print (\"File saved: \" + cmems_name.format(date.year, date.month, date.day))\n","      successful_downloads.append(\"{}_{}_{}\".format(date.year, date.month, date.day))\n","      download_count += 1\n","    except:\n","      print(\"Download failed: \" + cmems_name.format(date.year, date.month, date.day))\n","      failed_downloads.append(\"{}_{}_{}\".format(date.year, date.month, date.day))\n","  date += dt.timedelta(days=1)\n","\n","# Disconnect from the FTP server\n","server.quit()\n","\n","# Save download logs to csv\n","with open(\"Successful_Downloads.txt\", 'w') as f:\n","  for item in successful_downloads:\n","    f.write(item + \"\\n\")\n","if len(failed_downloads) > 0:\n","  with open(\"Failed_Downloads.txt\", 'w') as f:\n","    for item in failed_downloads:\n","      f.write(item + \"\\n\")\n","    \n","# Update the user\n","if download_count > 0:\n","  print(\"Done Saving Files! {} new files downloaded.\".format(download_count))\n","else:\n","  print(\"No new files were downloaded.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading file number 1: 2020-01-01\n","File saved: NC Files/2020/1/2020_1_1.nc\n","Downloading file number 2: 2020-01-02\n","File saved: NC Files/2020/1/2020_1_2.nc\n","Downloading file number 3: 2020-01-03\n","File saved: NC Files/2020/1/2020_1_3.nc\n","Downloading file number 4: 2020-01-04\n","File saved: NC Files/2020/1/2020_1_4.nc\n","Downloading file number 5: 2020-01-05\n","File saved: NC Files/2020/1/2020_1_5.nc\n","Downloading file number 6: 2020-01-06\n","File saved: NC Files/2020/1/2020_1_6.nc\n","Downloading file number 7: 2020-01-07\n","File saved: NC Files/2020/1/2020_1_7.nc\n","Downloading file number 8: 2020-01-08\n","File saved: NC Files/2020/1/2020_1_8.nc\n","Downloading file number 9: 2020-01-09\n","File saved: NC Files/2020/1/2020_1_9.nc\n","Downloading file number 10: 2020-01-10\n","File saved: NC Files/2020/1/2020_1_10.nc\n","Downloading file number 11: 2020-01-11\n","File saved: NC Files/2020/1/2020_1_11.nc\n","Downloading file number 12: 2020-01-12\n","File saved: NC Files/2020/1/2020_1_12.nc\n","Downloading file number 13: 2020-01-13\n","File saved: NC Files/2020/1/2020_1_13.nc\n","Downloading file number 14: 2020-01-14\n","File saved: NC Files/2020/1/2020_1_14.nc\n","Downloading file number 15: 2020-01-15\n","File saved: NC Files/2020/1/2020_1_15.nc\n","Downloading file number 16: 2020-01-16\n","File saved: NC Files/2020/1/2020_1_16.nc\n","Downloading file number 17: 2020-01-17\n","File saved: NC Files/2020/1/2020_1_17.nc\n","Downloading file number 18: 2020-01-18\n","File saved: NC Files/2020/1/2020_1_18.nc\n","Downloading file number 19: 2020-01-19\n","File saved: NC Files/2020/1/2020_1_19.nc\n","Downloading file number 20: 2020-01-20\n","File saved: NC Files/2020/1/2020_1_20.nc\n","Downloading file number 21: 2020-01-21\n","File saved: NC Files/2020/1/2020_1_21.nc\n","Downloading file number 22: 2020-01-22\n","File saved: NC Files/2020/1/2020_1_22.nc\n","Downloading file number 23: 2020-01-23\n","File saved: NC Files/2020/1/2020_1_23.nc\n","Downloading file number 24: 2020-01-24\n","File saved: NC Files/2020/1/2020_1_24.nc\n","Downloading file number 25: 2020-01-25\n","File saved: NC Files/2020/1/2020_1_25.nc\n","Downloading file number 26: 2020-01-26\n","File saved: NC Files/2020/1/2020_1_26.nc\n","Downloading file number 27: 2020-01-27\n","File saved: NC Files/2020/1/2020_1_27.nc\n","Downloading file number 28: 2020-01-28\n","File saved: NC Files/2020/1/2020_1_28.nc\n","Downloading file number 29: 2020-01-29\n","File saved: NC Files/2020/1/2020_1_29.nc\n","Downloading file number 30: 2020-01-30\n","File saved: NC Files/2020/1/2020_1_30.nc\n","Downloading file number 31: 2020-01-31\n","File saved: NC Files/2020/1/2020_1_31.nc\n","Downloading file number 32: 2020-02-01\n","File saved: NC Files/2020/2/2020_2_1.nc\n","Downloading file number 33: 2020-02-02\n","File saved: NC Files/2020/2/2020_2_2.nc\n","Downloading file number 34: 2020-02-03\n","File saved: NC Files/2020/2/2020_2_3.nc\n","Downloading file number 35: 2020-02-04\n","File saved: NC Files/2020/2/2020_2_4.nc\n","Downloading file number 36: 2020-02-05\n","File saved: NC Files/2020/2/2020_2_5.nc\n","Downloading file number 37: 2020-02-06\n","File saved: NC Files/2020/2/2020_2_6.nc\n","Downloading file number 38: 2020-02-07\n","File saved: NC Files/2020/2/2020_2_7.nc\n","Downloading file number 39: 2020-02-08\n","File saved: NC Files/2020/2/2020_2_8.nc\n","Downloading file number 40: 2020-02-09\n","File saved: NC Files/2020/2/2020_2_9.nc\n","Downloading file number 41: 2020-02-10\n","File saved: NC Files/2020/2/2020_2_10.nc\n","Downloading file number 42: 2020-02-11\n","File saved: NC Files/2020/2/2020_2_11.nc\n","Downloading file number 43: 2020-02-12\n","File saved: NC Files/2020/2/2020_2_12.nc\n","Downloading file number 44: 2020-02-13\n","File saved: NC Files/2020/2/2020_2_13.nc\n","Downloading file number 45: 2020-02-14\n","File saved: NC Files/2020/2/2020_2_14.nc\n","Downloading file number 46: 2020-02-15\n","File saved: NC Files/2020/2/2020_2_15.nc\n","Downloading file number 47: 2020-02-16\n","File saved: NC Files/2020/2/2020_2_16.nc\n","Downloading file number 48: 2020-02-17\n","File saved: NC Files/2020/2/2020_2_17.nc\n","Downloading file number 49: 2020-02-18\n","File saved: NC Files/2020/2/2020_2_18.nc\n","Downloading file number 50: 2020-02-19\n","File saved: NC Files/2020/2/2020_2_19.nc\n","Downloading file number 51: 2020-02-20\n","File saved: NC Files/2020/2/2020_2_20.nc\n","Downloading file number 52: 2020-02-21\n","File saved: NC Files/2020/2/2020_2_21.nc\n","Downloading file number 53: 2020-02-22\n","File saved: NC Files/2020/2/2020_2_22.nc\n","Downloading file number 54: 2020-02-23\n","File saved: NC Files/2020/2/2020_2_23.nc\n","Downloading file number 55: 2020-02-24\n","File saved: NC Files/2020/2/2020_2_24.nc\n","Downloading file number 56: 2020-02-25\n","File saved: NC Files/2020/2/2020_2_25.nc\n","Downloading file number 57: 2020-02-26\n","File saved: NC Files/2020/2/2020_2_26.nc\n","Downloading file number 58: 2020-02-27\n","File saved: NC Files/2020/2/2020_2_27.nc\n","Downloading file number 59: 2020-02-28\n","File saved: NC Files/2020/2/2020_2_28.nc\n","Downloading file number 60: 2020-02-29\n","File saved: NC Files/2020/2/2020_2_29.nc\n","Downloading file number 61: 2020-03-01\n","File saved: NC Files/2020/3/2020_3_1.nc\n","Downloading file number 62: 2020-03-02\n","File saved: NC Files/2020/3/2020_3_2.nc\n","Downloading file number 63: 2020-03-03\n","File saved: NC Files/2020/3/2020_3_3.nc\n","Downloading file number 64: 2020-03-04\n","File saved: NC Files/2020/3/2020_3_4.nc\n","Downloading file number 65: 2020-03-05\n","File saved: NC Files/2020/3/2020_3_5.nc\n","Downloading file number 66: 2020-03-06\n","File saved: NC Files/2020/3/2020_3_6.nc\n","Downloading file number 67: 2020-03-07\n","File saved: NC Files/2020/3/2020_3_7.nc\n","Downloading file number 68: 2020-03-08\n","File saved: NC Files/2020/3/2020_3_8.nc\n","Downloading file number 69: 2020-03-09\n","File saved: NC Files/2020/3/2020_3_9.nc\n","Downloading file number 70: 2020-03-10\n","File saved: NC Files/2020/3/2020_3_10.nc\n","Downloading file number 71: 2020-03-11\n","File saved: NC Files/2020/3/2020_3_11.nc\n","Downloading file number 72: 2020-03-12\n","File saved: NC Files/2020/3/2020_3_12.nc\n","Downloading file number 73: 2020-03-13\n","File saved: NC Files/2020/3/2020_3_13.nc\n","Downloading file number 74: 2020-03-14\n","File saved: NC Files/2020/3/2020_3_14.nc\n","Downloading file number 75: 2020-03-15\n","File saved: NC Files/2020/3/2020_3_15.nc\n","Downloading file number 76: 2020-03-16\n","File saved: NC Files/2020/3/2020_3_16.nc\n","Downloading file number 77: 2020-03-17\n","File saved: NC Files/2020/3/2020_3_17.nc\n","Downloading file number 78: 2020-03-18\n","File saved: NC Files/2020/3/2020_3_18.nc\n","Downloading file number 79: 2020-03-19\n","File saved: NC Files/2020/3/2020_3_19.nc\n","Downloading file number 80: 2020-03-20\n","File saved: NC Files/2020/3/2020_3_20.nc\n","Downloading file number 81: 2020-03-21\n","File saved: NC Files/2020/3/2020_3_21.nc\n","Downloading file number 82: 2020-03-22\n","File saved: NC Files/2020/3/2020_3_22.nc\n","Downloading file number 83: 2020-03-23\n","File saved: NC Files/2020/3/2020_3_23.nc\n","Downloading file number 84: 2020-03-24\n","File saved: NC Files/2020/3/2020_3_24.nc\n","Downloading file number 85: 2020-03-25\n","File saved: NC Files/2020/3/2020_3_25.nc\n","Downloading file number 86: 2020-03-26\n","File saved: NC Files/2020/3/2020_3_26.nc\n","Downloading file number 87: 2020-03-27\n","File saved: NC Files/2020/3/2020_3_27.nc\n","Downloading file number 88: 2020-03-28\n","File saved: NC Files/2020/3/2020_3_28.nc\n","Downloading file number 89: 2020-03-29\n","File saved: NC Files/2020/3/2020_3_29.nc\n","Downloading file number 90: 2020-03-30\n","File saved: NC Files/2020/3/2020_3_30.nc\n","Downloading file number 91: 2020-03-31\n","File saved: NC Files/2020/3/2020_3_31.nc\n","Downloading file number 92: 2020-04-01\n","File saved: NC Files/2020/4/2020_4_1.nc\n","Downloading file number 93: 2020-04-02\n","File saved: NC Files/2020/4/2020_4_2.nc\n","Downloading file number 94: 2020-04-03\n","File saved: NC Files/2020/4/2020_4_3.nc\n","Downloading file number 95: 2020-04-04\n","File saved: NC Files/2020/4/2020_4_4.nc\n","Downloading file number 96: 2020-04-05\n","File saved: NC Files/2020/4/2020_4_5.nc\n","Downloading file number 97: 2020-04-06\n","File saved: NC Files/2020/4/2020_4_6.nc\n","Downloading file number 98: 2020-04-07\n","File saved: NC Files/2020/4/2020_4_7.nc\n","Downloading file number 99: 2020-04-08\n","File saved: NC Files/2020/4/2020_4_8.nc\n","Downloading file number 100: 2020-04-09\n","File saved: NC Files/2020/4/2020_4_9.nc\n","Downloading file number 101: 2020-04-10\n","File saved: NC Files/2020/4/2020_4_10.nc\n","Downloading file number 102: 2020-04-11\n","File saved: NC Files/2020/4/2020_4_11.nc\n","Downloading file number 103: 2020-04-12\n","File saved: NC Files/2020/4/2020_4_12.nc\n","Downloading file number 104: 2020-04-13\n","File saved: NC Files/2020/4/2020_4_13.nc\n","Downloading file number 105: 2020-04-14\n","File saved: NC Files/2020/4/2020_4_14.nc\n","Downloading file number 106: 2020-04-15\n","File saved: NC Files/2020/4/2020_4_15.nc\n","Downloading file number 107: 2020-04-16\n","File saved: NC Files/2020/4/2020_4_16.nc\n","Downloading file number 108: 2020-04-17\n","File saved: NC Files/2020/4/2020_4_17.nc\n","Downloading file number 109: 2020-04-18\n","File saved: NC Files/2020/4/2020_4_18.nc\n","Downloading file number 110: 2020-04-19\n","File saved: NC Files/2020/4/2020_4_19.nc\n","Downloading file number 111: 2020-04-20\n","File saved: NC Files/2020/4/2020_4_20.nc\n","Downloading file number 112: 2020-04-21\n","File saved: NC Files/2020/4/2020_4_21.nc\n","Downloading file number 113: 2020-04-22\n","File saved: NC Files/2020/4/2020_4_22.nc\n","Downloading file number 114: 2020-04-23\n","File saved: NC Files/2020/4/2020_4_23.nc\n","Downloading file number 115: 2020-04-24\n","File saved: NC Files/2020/4/2020_4_24.nc\n","Downloading file number 116: 2020-04-25\n","File saved: NC Files/2020/4/2020_4_25.nc\n","Downloading file number 117: 2020-04-26\n","File saved: NC Files/2020/4/2020_4_26.nc\n","Downloading file number 118: 2020-04-27\n","File saved: NC Files/2020/4/2020_4_27.nc\n","Downloading file number 119: 2020-04-28\n","File saved: NC Files/2020/4/2020_4_28.nc\n","Downloading file number 120: 2020-04-29\n","File saved: NC Files/2020/4/2020_4_29.nc\n","Downloading file number 121: 2020-04-30\n","File saved: NC Files/2020/4/2020_4_30.nc\n","Downloading file number 122: 2020-05-01\n","File saved: NC Files/2020/5/2020_5_1.nc\n","Downloading file number 123: 2020-05-02\n","File saved: NC Files/2020/5/2020_5_2.nc\n","Downloading file number 124: 2020-05-03\n","File saved: NC Files/2020/5/2020_5_3.nc\n","Downloading file number 125: 2020-05-04\n","File saved: NC Files/2020/5/2020_5_4.nc\n","Downloading file number 126: 2020-05-05\n","File saved: NC Files/2020/5/2020_5_5.nc\n","Downloading file number 127: 2020-05-06\n","File saved: NC Files/2020/5/2020_5_6.nc\n","Downloading file number 128: 2020-05-07\n","File saved: NC Files/2020/5/2020_5_7.nc\n","Downloading file number 129: 2020-05-08\n","File saved: NC Files/2020/5/2020_5_8.nc\n","Downloading file number 130: 2020-05-09\n","File saved: NC Files/2020/5/2020_5_9.nc\n","Downloading file number 131: 2020-05-10\n","File saved: NC Files/2020/5/2020_5_10.nc\n","Downloading file number 132: 2020-05-11\n","File saved: NC Files/2020/5/2020_5_11.nc\n","Downloading file number 133: 2020-05-12\n","File saved: NC Files/2020/5/2020_5_12.nc\n","Downloading file number 134: 2020-05-13\n","File saved: NC Files/2020/5/2020_5_13.nc\n","Downloading file number 135: 2020-05-14\n","File saved: NC Files/2020/5/2020_5_14.nc\n","Downloading file number 136: 2020-05-15\n","File saved: NC Files/2020/5/2020_5_15.nc\n","Downloading file number 137: 2020-05-16\n","File saved: NC Files/2020/5/2020_5_16.nc\n","Downloading file number 138: 2020-05-17\n","File saved: NC Files/2020/5/2020_5_17.nc\n","Downloading file number 139: 2020-05-18\n","File saved: NC Files/2020/5/2020_5_18.nc\n","Downloading file number 140: 2020-05-19\n","File saved: NC Files/2020/5/2020_5_19.nc\n","Downloading file number 141: 2020-05-20\n","File saved: NC Files/2020/5/2020_5_20.nc\n","Downloading file number 142: 2020-05-21\n","File saved: NC Files/2020/5/2020_5_21.nc\n","Downloading file number 143: 2020-05-22\n","File saved: NC Files/2020/5/2020_5_22.nc\n","Downloading file number 144: 2020-05-23\n","File saved: NC Files/2020/5/2020_5_23.nc\n","Downloading file number 145: 2020-05-24\n","File saved: NC Files/2020/5/2020_5_24.nc\n","Downloading file number 146: 2020-05-25\n","File saved: NC Files/2020/5/2020_5_25.nc\n","Downloading file number 147: 2020-05-26\n","File saved: NC Files/2020/5/2020_5_26.nc\n","Downloading file number 148: 2020-05-27\n","File saved: NC Files/2020/5/2020_5_27.nc\n","Downloading file number 149: 2020-05-28\n","File saved: NC Files/2020/5/2020_5_28.nc\n","Downloading file number 150: 2020-05-29\n","File saved: NC Files/2020/5/2020_5_29.nc\n","Downloading file number 151: 2020-05-30\n","File saved: NC Files/2020/5/2020_5_30.nc\n","Downloading file number 152: 2020-05-31\n","File saved: NC Files/2020/5/2020_5_31.nc\n","Downloading file number 153: 2020-06-01\n","File saved: NC Files/2020/6/2020_6_1.nc\n","Downloading file number 154: 2020-06-02\n","File saved: NC Files/2020/6/2020_6_2.nc\n","Downloading file number 155: 2020-06-03\n","File saved: NC Files/2020/6/2020_6_3.nc\n","Downloading file number 156: 2020-06-04\n","File saved: NC Files/2020/6/2020_6_4.nc\n","Downloading file number 157: 2020-06-05\n","File saved: NC Files/2020/6/2020_6_5.nc\n","Downloading file number 158: 2020-06-06\n","File saved: NC Files/2020/6/2020_6_6.nc\n","Downloading file number 159: 2020-06-07\n","File saved: NC Files/2020/6/2020_6_7.nc\n","Downloading file number 160: 2020-06-08\n","File saved: NC Files/2020/6/2020_6_8.nc\n","Downloading file number 161: 2020-06-09\n","File saved: NC Files/2020/6/2020_6_9.nc\n","Downloading file number 162: 2020-06-10\n","File saved: NC Files/2020/6/2020_6_10.nc\n","Downloading file number 163: 2020-06-11\n","File saved: NC Files/2020/6/2020_6_11.nc\n","Downloading file number 164: 2020-06-12\n","File saved: NC Files/2020/6/2020_6_12.nc\n","Downloading file number 165: 2020-06-13\n","File saved: NC Files/2020/6/2020_6_13.nc\n","Downloading file number 166: 2020-06-14\n","File saved: NC Files/2020/6/2020_6_14.nc\n","Downloading file number 167: 2020-06-15\n","File saved: NC Files/2020/6/2020_6_15.nc\n","Downloading file number 168: 2020-06-16\n","File saved: NC Files/2020/6/2020_6_16.nc\n","Downloading file number 169: 2020-06-17\n","File saved: NC Files/2020/6/2020_6_17.nc\n","Downloading file number 170: 2020-06-18\n","File saved: NC Files/2020/6/2020_6_18.nc\n","Downloading file number 171: 2020-06-19\n","File saved: NC Files/2020/6/2020_6_19.nc\n","Downloading file number 172: 2020-06-20\n","File saved: NC Files/2020/6/2020_6_20.nc\n","Downloading file number 173: 2020-06-21\n","File saved: NC Files/2020/6/2020_6_21.nc\n","Downloading file number 174: 2020-06-22\n","File saved: NC Files/2020/6/2020_6_22.nc\n","Downloading file number 175: 2020-06-23\n","File saved: NC Files/2020/6/2020_6_23.nc\n","Downloading file number 176: 2020-06-24\n","File saved: NC Files/2020/6/2020_6_24.nc\n","Downloading file number 177: 2020-06-25\n","File saved: NC Files/2020/6/2020_6_25.nc\n","Downloading file number 178: 2020-06-26\n","File saved: NC Files/2020/6/2020_6_26.nc\n","Downloading file number 179: 2020-06-27\n","File saved: NC Files/2020/6/2020_6_27.nc\n","Downloading file number 180: 2020-06-28\n","File saved: NC Files/2020/6/2020_6_28.nc\n","Downloading file number 181: 2020-06-29\n","File saved: NC Files/2020/6/2020_6_29.nc\n","Done Saving Files! 181 new files downloaded.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ErULNVtViVZI"},"source":["## Delete all CMEMS data - Be careful!"]},{"cell_type":"code","metadata":{"id":"JUu1hLsXiami"},"source":["os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if cmems_label_folder in os.listdir():\n","  os.chdir(cmems_label_folder)\n","\n","  if \"Successful_Downloads.txt\" in os.listdir(): os.remove(\"Successful_Downloads.txt\")\n","  if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n","\n","  if \"NC Files\" in os.listdir():\n","    for y in os.listdir(\"NC Files\"):\n","      for m in os.listdir(os.path.join(\"NC Files\", y)):\n","        for f in os.listdir(os.path.join(\"NC Files\", y, m)):\n","          os.remove(os.path.join(\"NC Files\", y, m, f))\n","        os.rmdir(os.path.join(\"NC Files\", y, m))\n","      os.rmdir(os.path.join(\"NC Files\", y))\n","    os.rmdir(\"NC Files\")\n","\n","  if len(os.listdir()) == 0:\n","    os.chdir(\"..\")\n","    os.rmdir(cmems_label_folder)\n","\n","  print(\"All files should have been deleted.\")\n","else:\n","  print(\"No CMEMS data to delete.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVaBydhfo37f"},"source":["# Step 4: Patch-wise Labelling"]},{"cell_type":"code","metadata":{"id":"e3ACY67yZKYL","cellView":"form","executionInfo":{"status":"ok","timestamp":1604004398882,"user_tz":-120,"elapsed":4935527,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"dad05ade-34a4-4d28-aacf-af9122fbca57","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title Labelling\n","def generate_label(year, month, day, image, odata, nc_file):\n","  # Get the footprint information, so that we can interpolate the coordinates of each pixel\n","  # NOTE: This uses the 'footprint' provided by OData, in which the order of the points has been consistently\n","  # manipulated along with the image, so the order of the points is consistent in the satellite frame of reference.\n","  # This is different to the OpenSearch 'footprint', where the points are ordered in the global frame, and thus\n","  # do not follow any flips/rotations performed to the image as part of the ESA preprocessing.\n","  footprint = odata['footprint'][9:-2].split(\",\")\n","  footprint = [(float(footprint[i].split(\" \")[1]), float(footprint[i].split(\" \")[0])) for i in range(4)]\n","\n","  # The OData 'footprint' is always ordered in according to the satellite frame\n","  # of reference. When the images are flipped/rotated in preprocessing, the\n","  # 'footprint' points remain ordered in the satellite frame orietation. The\n","  # 'footprint' consists of 5 points: [(0), (1), (2), (3), (0)], and they\n","  # are ordered as follows (in the satellite frame):\n","  #\n","  #                 (0)                 (1)\n","  #                     +-------------+\n","  #                     |             |\n","  #                     |             |\n","  #                     |             |\n","  #                     |             |\n","  #                     +-------------+\n","  #                 (3)                 (2)\n","  #\n","  # So, if the latitude of (1) is greater than the latitude of (2), then the\n","  # image was acquired during an ASCENDING pass, otherwise it was during a\n","  # DESCENDING pass. This can be used to determin the pass direction without\n","  # relying on the 'orbit direction' entry which seems to be unreliable in both\n","  # OData and OpenSearch APIs.\n","  if footprint[1][0] > footprint[2][0] and footprint[2][1] < footprint[0][1]:\n","    # The satellite was definitely ASCENDING, and the footprint straddles the date line\n","    for i in range(4): footprint[i] = (footprint[i][0], footprint[i][1] + (footprint[i][1] // -360) * 360)\n","  elif footprint[2][0] > footprint[1][0] and footprint[3][1] < footprint[1][1]:\n","    # The satellite was definitely DESCENDING, and the footprint straddles the date line\n","    for i in range(4): footprint[i] = (footprint[i][0], footprint[i][1] + (footprint[i][1] // -360) * 360)\n","  \n","  top_left_lat     = footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2][0]\n","  top_left_lon     = footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2][1]\n","  top_right_lat    = footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3][0]\n","  top_right_lon    = footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3][1]\n","  bottom_right_lat = footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0][0]\n","  bottom_right_lon = footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0][1]\n","  bottom_left_lat  = footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1][0]\n","  bottom_left_lon  = footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1][1]\n","  # OData 'pass direction' works reliably in the section above, because any mistakes happen twice\n","  # and effectively cancel out (although the image and label will be 180 degrees rotated from the\n","  # global frame). To deal with the issue of crossing the date line, we need to know for sure which\n","  # direction the pass was in (ASCENDING/DESCENDING). This will figure that out, and correct for\n","  # negative lon values causing problems when interpolating.\n","  \n","  # Create linspace objects for left and right edges so that the rows can be iterated later\n","  left_edge  = [np.linspace(top_left_lat,  bottom_left_lat,  image_dimension), np.linspace(top_left_lon,  bottom_left_lon,  image_dimension)]\n","  right_edge = [np.linspace(top_right_lat, bottom_right_lat, image_dimension), np.linspace(top_right_lon, bottom_right_lon, image_dimension)]\n","\n","  # Extract the relevent tables from the .nc file\n","  full_concentration = nc_file.variables['ice_conc'][0, :, :]\n","  full_uncertainty   = nc_file.variables['total_uncertainty'][0, :, :]\n","  full_lat = np.array(nc_file.variables['lat'])\n","  full_lon = np.array(nc_file.variables['lon'])\n","\n","  # Compute the bounds of the image on the concentration map to reduce computational requirements later on\n","  index_values = np.zeros((4, 2), dtype=np.uint16)\n","  # top_left\n","  distance_from_pixel = (full_lat - top_left_lat)**2 + (full_lon - (top_left_lon - 360 * ((top_left_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n","  index = distance_from_pixel.argmin()\n","  index_values[0, 0], index_values[0, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n","  # top_right\n","  distance_from_pixel = (full_lat - top_right_lat)**2 + (full_lon - (top_right_lon - 360 * ((top_right_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n","  index = distance_from_pixel.argmin()\n","  index_values[1, 0], index_values[1, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n","  # bottom_left\n","  distance_from_pixel = (full_lat - bottom_left_lat)**2 + (full_lon - (bottom_left_lon - 360 * ((bottom_left_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n","  index = distance_from_pixel.argmin()\n","  index_values[2, 0], index_values[2, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n","  # bottom_right\n","  distance_from_pixel = (full_lat - bottom_right_lat)**2 + (full_lon - (bottom_right_lon - 360 * ((bottom_right_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n","  index = distance_from_pixel.argmin()\n","  index_values[3, 0], index_values[3, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n","  # Find the reduced size arrays\n","  min_y = min(index_values[:, 0])\n","  min_x = min(index_values[:, 1])\n","  max_y = max(index_values[:, 0])\n","  max_x = max(index_values[:, 1])\n","  # Slice the full arrays\n","  reduced_concentration = full_concentration[min_y:max_y, min_x:max_x]\n","  reduced_uncertainty   = full_uncertainty[min_y:max_y, min_x:max_x]\n","  reduced_lat = full_lat[min_y:max_y, min_x:max_x]\n","  reduced_lon = full_lon[min_y:max_y, min_x:max_x]\n","\n","  # Create the label arrays\n","  concentration_label = np.zeros((image_dimension, image_dimension), dtype=np.float_)\n","  uncertainty_label   = np.zeros((image_dimension, image_dimension), dtype=np.float_)\n","\n","  # Iterate through each pixel and find the corresponding value for the label\n","  for y_pixel in range(image_dimension):\n","    interpolated_row = [np.linspace(left_edge[0][y_pixel], right_edge[0][y_pixel], image_dimension), np.linspace(left_edge[1][y_pixel], right_edge[1][y_pixel], image_dimension)]\n","    for x_pixel in range(image_dimension):\n","      if image[y_pixel, x_pixel] != 0:\n","        # Only compute the concentration for non-zero pixels in the image (i.e. ignore black edges)\n","        pixel_lat = interpolated_row[0][x_pixel]\n","        pixel_lon = interpolated_row[1][x_pixel]\n","        # Find the closest coordinate to this pixel, and use its value\n","        distance_from_pixel = (reduced_lat - pixel_lat)**2 + (reduced_lon - (pixel_lon - 360 * ((pixel_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n","        index = distance_from_pixel.argmin()\n","        y, x = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n","        concentration_label[y_pixel, x_pixel] = 100 if reduced_concentration.mask[y, x] else reduced_concentration[y, x]\n","        uncertainty_label[y_pixel, x_pixel]   = 0 if reduced_uncertainty.mask[y, x] else reduced_uncertainty[y, x]\n","\n","  # Save the label patches\n","  os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","  if not os.path.exists(\"Concentration Labels/{}/{}/{}\".format(year, month, day)): os.makedirs(\"Concentration Labels/{}/{}/{}\".format(year, month, day))\n","  if not os.path.exists(\"Uncertainty Labels/{}/{}/{}\".format(year, month, day)): os.makedirs(\"Uncertainty Labels/{}/{}/{}\".format(year, month, day))\n","  cv2.imwrite(concentration_name.format(year, month, day, odata['id']), concentration_label)\n","  cv2.imwrite(uncertainty_name.format(year, month, day, odata['id']), uncertainty_label)\n","\n","#------------------------------------------------------------------------------#\n","\n","# Read in the list of successfully downloaded images\n","os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","if \"Successful_Downloads.txt\" in os.listdir():\n","  with open(\"Successful_Downloads.txt\", 'r') as file:\n","    needs_labelling = file.read().splitlines()\n","  print(\"Found list of all successfully downloaded images.\")\n","else:\n","  needs_labelling = []\n","  print(\"Cannot find Successful_Downloads file for SENTINEL-1 images.\")\n","\n","# Check for successful label file\n","os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","if \"Successful_Labels.txt\" in os.listdir():\n","  # If some labelling has been done, remove the already lebelled images from the list to avoid reprocessing\n","  with open(\"Successful_Labels.txt\", 'r') as file:\n","    already_labelled = file.read().splitlines()\n","  for item in already_labelled:\n","    if item in needs_labelling:\n","      needs_labelling.remove(item)\n","  print(\"Removed all images which were already labelled.\")\n","else:\n","  already_labelled = []\n","  print(\"No images have been labelled yet.\")\n","\n","# Delete the failed labels file so that they can be retried and updated as necessary\n","if \"Failed_Labels.txt\" in os.listdir(): os.remove(\"Failed_Labels.txt\")\n","  \n","if len(needs_labelling) > 0:\n","  print(\"Attempting to label {} remaining images.\\n\".format(len(needs_labelling)))\n","  # Iterate through each image in needs_labelling\n","  for filename in needs_labelling:\n","    try:\n","      # Get the odata info (date, footprint, etc)\n","      year  = filename.split(\"_\")[0]\n","      month = filename.split(\"_\")[1]\n","      day   = filename.split(\"_\")[2]\n","      uuid  = filename.split(\"_\")[3]\n","      odata = search_api.get_product_odata(uuid, full=True)\n","\n","\n","      # Load the resampled image as grayscale\n","      os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","      image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(year, month, day, uuid)), cv2.COLOR_BGR2GRAY)\n","\n","      # Load the appropriate concentration map\n","      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","      nc_file = nc.Dataset(cmems_name.format(year, month, day), 'r', format=\"NETCDF3\")\n","      \n","      # Generate the label from the avaiable information\n","      generate_label(year, month, day, image, odata, nc_file)\n","\n","      # Log the success\n","      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","      with open(\"Successful_Labels.txt\", 'a+') as f:\n","        f.write(filename + \"\\n\")\n","      clear_output()\n","      print(\"Finished labelling {} of {}: {}\".format(needs_labelling.index(filename) + 1, len(needs_labelling), filename))\n","    except KeyboardInterrupt:\n","      raise\n","    except:\n","      # Log the failure\n","      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","      with open(\"Failed_Labels.txt\", 'a+') as f:\n","        f.write(filename + \"\\n\")\n","      clear_output()\n","      print(\"Labelling Failed: {}\".format(filename))\n","\n","  clear_output()\n","  print(\"Finished trying to generate all required labels.\")\n","\n","else:\n","  # Needs_labelling is empty\n","  print(\"\\nNo more images need to be labelled.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished trying to generate all required labels.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SfcA9sgUq5X0"},"source":["## Delete all labelling patches - Be careful!"]},{"cell_type":"code","metadata":{"id":"ccESOitEF9ak","executionInfo":{"status":"error","timestamp":1603739289598,"user_tz":-120,"elapsed":5635,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"b040556f-7c6d-4e43-9b85-e4eee3ea6b88","colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if cmems_label_folder in os.listdir():\n","  os.chdir(cmems_label_folder)\n","\n","  if \"Successful_Labels.txt\" in os.listdir(): os.remove(\"Successful_Labels.txt\")\n","  if \"Failed_Labels.txt\" in os.listdir(): os.remove(\"Failed_Labels.txt\")\n","\n","  if \"Concentration Labels\" in os.listdir():\n","    for y in os.listdir(\"Concentration Labels\"):\n","      for m in os.listdir(os.path.join(\"Concentration Labels\", y)):\n","        for d in os.listdir(os.path.join(\"Concentration Labels\", y, m)):\n","          for f in os.listdir(os.path.join(\"Concentration Labels\", y, m, d)):\n","            os.remove(os.path.join(\"Concentration Labels\", y, m, d, f))\n","          os.rmdir(os.path.join(\"Concentration Labels\", y, m, d))\n","        os.rmdir(os.path.join(\"Concentration Labels\", y, m))\n","      os.rmdir(os.path.join(\"Concentration Labels\", y))\n","    os.rmdir(\"Concentration Labels\")\n","\n","  if \"Uncertainty Labels\" in os.listdir():\n","    for y in os.listdir(\"Uncertainty Labels\"):\n","      for m in os.listdir(os.path.join(\"Uncertainty Labels\", y)):\n","        for d in os.listdir(os.path.join(\"Uncertainty Labels\", y, m)):\n","          for f in os.listdir(os.path.join(\"Uncertainty Labels\", y, m, d)):\n","            os.remove(os.path.join(\"Uncertainty Labels\", y, m, d, f))\n","          os.rmdir(os.path.join(\"Uncertainty Labels\", y, m, d))\n","        os.rmdir(os.path.join(\"Uncertainty Labels\", y, m))\n","      os.rmdir(os.path.join(\"Uncertainty Labels\", y))\n","    os.rmdir(\"Uncertainty Labels\")\n","\n","  if len(os.listdir()) == 0:\n","    os.chdir(\"..\")\n","    os.rmdir(cmems_label_folder)\n","\n","  print(\"All files should have been deleted.\")\n","else:\n","  print(\"No CMEMS data to delete.\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-7824d783db1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concentration Labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concentration Labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concentration Labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m           \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concentration Labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concentration Labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"0bkZiOEdoKpt"},"source":["# Step 5: Curate Samples"]},{"cell_type":"code","metadata":{"id":"NRu9qW3loTAm","cellView":"form","executionInfo":{"status":"ok","timestamp":1604004499438,"user_tz":-120,"elapsed":5028359,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"29af5502-dbab-47c0-9ce2-5f95bcb32b77","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title Curate Samples\n","# Function to decide whether or not to keep a sample\n","def keepSample(date_uuid):\n","  keep = True\n","  # Load the original image, to check the aspect ratio\n","  os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","  original = cv2.imread(sentinel_1_quicklook_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]))\n","  aspect_ratio = np.shape(original)[1] / np.shape(original)[0]\n","  if aspect_ratio < 0.8 or aspect_ratio > 1.2: keep = False\n","  \n","  if keep: # don't do unnecessary checks\n","    # Load the concentration label, to check the variance\n","    os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","    concentration = cv2.medianBlur(cv2.imread(concentration_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE), 5) / 100\n","    if np.var(concentration) < 0.05: keep = False\n","  return keep\n","#------------------------------------------------------------------------------#\n","\n","# Check if a list of samples is already there\n","os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if \"Samples.txt\" in os.listdir() and \"Ignored Samples.txt\" in os.listdir():\n","  with open(\"Samples.txt\", 'r') as f:\n","    included_samples = f.read().splitlines()\n","  with open(\"Ignored Samples.txt\", 'r') as f:\n","    ignored_samples = f.read().splitlines()\n","  already_sorted = included_samples + ignored_samples\n","  print(\"Found already sorted samples.\")\n","else:\n","  already_sorted = []\n","  print(\"Nothing previously sorted.\")\n","\n","# Get list of labeled images\n","os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","if \"Successful_Labels.txt\" in os.listdir():\n","  with open(\"Successful_Labels.txt\", 'r') as f:\n","    date_uuids = f.read().splitlines()\n","\n","  # Remove samples already sorted\n","  for date_uuid in already_sorted:\n","    date_uuids.remove(date_uuid)\n","\n","  # Decide which samples to keep\n","  os.chdir(os.path.join(dataset_location_path, dataset_name))\n","  for date_uuid in date_uuids:\n","    clear_output()\n","    print(\"Evaluating sample {} of {}: {}\".format(date_uuids.index(date_uuid) + 1, len(date_uuids), date_uuid))\n","    if keepSample(date_uuid):\n","      os.chdir(os.path.join(dataset_location_path, dataset_name))\n","      with open(\"Samples.txt\", 'a+') as f:\n","        f.write(date_uuid + \"\\n\")\n","    else:\n","      os.chdir(os.path.join(dataset_location_path, dataset_name))\n","      with open(\"Ignored Samples.txt\", 'a+') as f:\n","        f.write(date_uuid + \"\\n\")\n","  \n","  # Update the user\n","  os.chdir(os.path.join(dataset_location_path, dataset_name))\n","  print(\"\\nSamples.txt file saved with all included samples.\")\n","  with open(\"Ignored Samples.txt\", 'r') as f:\n","    ignored_samples = f.read().splitlines()\n","  with open(\"Samples.txt\", 'r') as f:\n","    samples = f.read().splitlines()\n","  print(\"{} samples included, out of {} in total.\".format(len(samples), len(samples) + len(ignored_samples)))\n","  print(\"All excluded samples were listed in the Ignored Samples txt file.\")\n","else:\n","  print(\"Successful_Labels.txt could not be found.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Evaluating sample 5293 of 5293: 2020_1_1_ec3c6c6a-e47f-4e1b-9b26-303cf14a4cad\n","\n","Samples.txt file saved with all included samples.\n","1867 samples included, out of 5293 in total.\n","All excluded samples were listed in the Ignored Samples txt file.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gMzckDO-4prs"},"source":["### Delete curated sample lists - Be careful!"]},{"cell_type":"code","metadata":{"id":"_p0uer8A4v7f","executionInfo":{"status":"ok","timestamp":1600132602621,"user_tz":-120,"elapsed":1101,"user":{"displayName":"Muneeb Bray","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv-WT-XIExonrmdCH8H273IpPsxYm_aX43K4Ff=s64","userId":"11618667995312436376"}},"outputId":"1d360e17-6ef8-483c-9585-1dcec05d7604","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if \"Samples.txt\" in os.listdir(): os.remove(\"Samples.txt\")\n","if \"Ignored Samples.txt\" in os.listdir(): os.remove(\"Ignored Samples.txt\")\n","\n","print(\"All curated sample lists should have been deleted.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["All curated sample lists should have been deleted.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J_eoZospiWwc"},"source":["# Step 6: Generate Batch Sets"]},{"cell_type":"code","metadata":{"id":"jAgFA0Ax7cS9","cellView":"form","executionInfo":{"status":"ok","timestamp":1604004582847,"user_tz":-120,"elapsed":1297,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"7f892d0f-288a-45fc-f0e4-55aa7ed65f26","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title ## Get all existing batch sets\n","#seed = 1\n","# Function to split the dataset and save the txt files\n","def split_dataset():\n","  print(\"Splitting dataset now.\")\n","  os.chdir(os.path.join(dataset_location_path, dataset_name))\n","  \n","  # Read in the 'good' samples, and split for train/test\n","  samples_dataframe      = pd.read_csv(\"Samples.txt\", header=None, names=[\"date_uuid\"])\n","  test_dataframe         = samples_dataframe.sample(frac=0.2, random_state=seed)\n","  train_dataframe        = samples_dataframe.drop(test_dataframe.index)\n","\n","  # Read in the 'bad' samples, and select a number of them for testing later on\n","  test_ignored_dataframe = pd.read_csv(\"Ignored Samples.txt\", header=None, names=[\"date_uuid\"])\n","  test_ignored_dataframe = test_ignored_dataframe.sample(n=min(100, len(test_ignored_dataframe['date_uuid'])), random_state=seed)\n","\n","  # Print a summary\n","  print(\"Dataset has just been split.\\n{} for training, {} for testing, and {} from the ignored samples.\".format(len(train_dataframe['date_uuid']), len(test_dataframe['date_uuid']), len(test_ignored_dataframe['date_uuid'])))\n","\n","  # Save the split lists to file\n","  os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n","  train_dataframe.to_csv(\"Train.txt\", header=False, index=False, mode='w')\n","  test_dataframe.to_csv(\"Test.txt\", header=False, index=False, mode='w')\n","  test_ignored_dataframe.to_csv(\"Test_Ignored.txt\", header=False, index=False, mode='w')\n","  print(\"The split sets have been saved to file for future use.\")\n","\n","# Function to generate and save the image batches\n","def generate_batches(augmentation_args):\n","  #batch_size = 50\n","  # Image preprocessing functions applied at batch time\n","  def image_preprocessing(img):\n","    return img\n","\n","  def concentration_preprocessing(img):\n","    img[:, :, 0] = cv2.medianBlur(img, 5)\n","    return img\n","\n","  def uncertainty_preprocessing(img):\n","    img[:, :, 0] = cv2.medianBlur(img, 5)\n","    return img\n","  \n","  # Add a column for the file paths, generated from the file name\n","  train_dataframe[\"path\"]         = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in train_dataframe['date_uuid']]\n","  test_dataframe[\"path\"]          = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_dataframe['date_uuid']]\n","  test_ignored_dataframe[\"path\"]  = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_ignored_dataframe['date_uuid']]\n","\n","  # Create DataGenerator objects\n","  train_image_datagen               = ImageDataGenerator(rescale=1./255, **augmentation_args, preprocessing_function=image_preprocessing)\n","  train_label_datagen               = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=concentration_preprocessing)\n","  train_uncertainty_datagen         = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=uncertainty_preprocessing)\n","\n","  test_image_datagen                = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n","  test_label_datagen                = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n","  test_uncertainty_datagen          = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n","\n","  test_ignored_image_datagen        = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n","  test_ignored_label_datagen        = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n","  test_ignored_uncertainty_datagen  = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n","\n","  # Set the DataGens to flow from the dataframe paths\n","  train_image_generator              =              train_image_datagen.flow_from_dataframe(train_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"rgb\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False,\n","                                                                                            seed=seed)\n","  train_label_generator              =              train_label_datagen.flow_from_dataframe(train_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False,\n","                                                                                            seed=seed)\n","  train_uncertainty_generator        =        train_uncertainty_datagen.flow_from_dataframe(train_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False,\n","                                                                                            seed=seed)\n","  \n","  test_image_generator               =               test_image_datagen.flow_from_dataframe(test_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"rgb\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","  test_label_generator               =               test_label_datagen.flow_from_dataframe(test_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","  test_uncertainty_generator         =         test_uncertainty_datagen.flow_from_dataframe(test_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","  \n","  test_ignored_image_generator       =       test_ignored_image_datagen.flow_from_dataframe(test_ignored_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"rgb\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","  test_ignored_label_generator       =       test_ignored_label_datagen.flow_from_dataframe(test_ignored_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","  test_ignored_uncertainty_generator = test_ignored_uncertainty_datagen.flow_from_dataframe(test_ignored_dataframe,\n","                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n","                                                                                            x_col=\"path\",\n","                                                                                            target_size=(image_dimension, image_dimension),\n","                                                                                            color_mode=\"grayscale\",\n","                                                                                            class_mode=None,\n","                                                                                            batch_size=batch_size,\n","                                                                                            shuffle=False)\n","\n","  # Update the user\n","  print(\"Finished preparing DataGenerator objects.\")\n","\n","  # Iterate through all the batches and save to file\n","  generators = {\"Train\"        : (train_image_generator,        train_label_generator,        train_uncertainty_generator),\n","                \"Test\"         : (test_image_generator,         test_label_generator,         test_uncertainty_generator),\n","                \"Test_Ignored\" : (test_ignored_image_generator, test_ignored_label_generator, test_ignored_uncertainty_generator)}\n","\n","  # Iterate through each segment (train, val, etc...)\n","  for segment, gen_tuple in generators.items():\n","    # Update the user\n","    print(\"\\nDealing with {} now.\".format(segment))\n","    \n","    # Move into the folder for this segment\n","    os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\", batch_name))\n","    if segment not in os.listdir(): os.mkdir(segment)\n","    os.chdir(segment)\n","\n","    # Find the number of digits required to count the number of batches\n","    digits = int(np.ceil(np.log10(len(gen_tuple[0]))))\n","\n","    # Iterate through each batch, and save the batch's data\n","    for batch_number in range(len(gen_tuple[0])):\n","      batch_data = {\"images\"        : np.array(next(gen_tuple[0])),\n","                    \"labels\"        : np.array(next(gen_tuple[1]), ),\n","                    \"uncertainties\" : np.array(next(gen_tuple[2]))}\n","      filename = (\"{:0\" + str(digits) +  \"}.npz\").format(batch_number)\n","      if filename not in os.listdir():\n","        np.savez(filename, **batch_data)\n","        print(\"Saved batch {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n","      else:\n","        print(\"Batch already saved: {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n","\n","  print(\"\\nFinished saving batches.\")\n","\n","#------------------------------------------------------------------------------#\n","\n","# Names for each batch segemnt\n","batch_segments = [\"Train\", \"Test\", \"Test_Ignored\"]\n","\n","# Set data augmentation parameters\n","augmentation_args = {\"rotation_range\" : 10,\n","                     \"fill_mode\"        : \"constant\",\n","                     \"cval\"             : 0,\n","                     \"horizontal_flip\"  : True,\n","                     \"vertical_flip\"    : True}\n","\n","# Get list of available batch sets\n","os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if \"Batches\" in os.listdir():\n","  os.chdir(\"Batches\")\n","  if len(os.listdir()) > len(batch_segments):\n","    print(\"Here is a list of all available batch sets:\")\n","    for d in os.listdir():\n","      if os.path.isdir(d): print(\"\\t\" + d)\n","    print(\"Please choose one and enter its name into the following cell. Otherwise create a new batch set below.\")\n","  else:\n","    print(\"No batches available. Please create a new batch set below.\")\n","else:\n","  os.mkdir(\"Batches\")\n","  split_dataset()\n","  print(\"\\nNo batches available. Please create a new batch set below.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Splitting dataset now.\n","Dataset has just been split.\n","1494 for training, 373 for testing, and 100 from the ignored samples.\n","The split sets have been saved to file for future use.\n","\n","No batches available. Please create a new batch set below.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-WF51geiXqXf","cellView":"form","executionInfo":{"status":"ok","timestamp":1604004629441,"user_tz":-120,"elapsed":25698,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"6cc68c10-77e2-4e13-fd72-d85bb228a86e","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title ## Create new batch set\n","batch_name = \"Southern_Melting_3\" #@param {type:\"string\"}\n","use_augmentation = False #@param {type:\"boolean\"}\n","if use_augmentation: batch_name += \"_A\"\n","#------------------------------------------------------------------------------#\n","\n","# Check that the batch name is unique\n","os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n","if batch_name in os.listdir():\n","  print(\"This batch set name already exists. Please chose a unique name if you want to create a new batch set.\")\n","else:\n","  # Check if all three split files are here\n","  if not all([\"{}.txt\".format(s) in os.listdir() for s in batch_segments]):\n","    # Missing split files. Make them\n","    print(\"Warning: One or more of the split files was missing. It will be generated now.\")\n","    split_dataset()\n","  else:\n","    print(\"Dataset has already been split. To re-split, delete the txt files and run this code again.\")\n","  \n","  # Load the dataset splits\n","  train_dataframe        = pd.read_csv(\"Train.txt\", header=None, names=['date_uuid'])\n","  test_dataframe         = pd.read_csv(\"Test.txt\", header=None, names=['date_uuid'])\n","  test_ignored_dataframe = pd.read_csv(\"Test_Ignored.txt\", header=None, names=['date_uuid'])\n","  \n","  # Generate the batch set\n","  os.mkdir(batch_name)\n","  generate_batches(augmentation_args if use_augmentation else {})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset has already been split. To re-split, delete the txt files and run this code again.\n","Found 1494 validated image filenames.\n","Found 1494 validated image filenames.\n","Found 1494 validated image filenames.\n","Found 373 validated image filenames.\n","Found 373 validated image filenames.\n","Found 373 validated image filenames.\n","Found 100 validated image filenames.\n","Found 100 validated image filenames.\n","Found 100 validated image filenames.\n","Finished preparing DataGenerator objects.\n","\n","Dealing with Train now.\n","Saved batch 1 of 94\n","Saved batch 2 of 94\n","Saved batch 3 of 94\n","Saved batch 4 of 94\n","Saved batch 5 of 94\n","Saved batch 6 of 94\n","Saved batch 7 of 94\n","Saved batch 8 of 94\n","Saved batch 9 of 94\n","Saved batch 10 of 94\n","Saved batch 11 of 94\n","Saved batch 12 of 94\n","Saved batch 13 of 94\n","Saved batch 14 of 94\n","Saved batch 15 of 94\n","Saved batch 16 of 94\n","Saved batch 17 of 94\n","Saved batch 18 of 94\n","Saved batch 19 of 94\n","Saved batch 20 of 94\n","Saved batch 21 of 94\n","Saved batch 22 of 94\n","Saved batch 23 of 94\n","Saved batch 24 of 94\n","Saved batch 25 of 94\n","Saved batch 26 of 94\n","Saved batch 27 of 94\n","Saved batch 28 of 94\n","Saved batch 29 of 94\n","Saved batch 30 of 94\n","Saved batch 31 of 94\n","Saved batch 32 of 94\n","Saved batch 33 of 94\n","Saved batch 34 of 94\n","Saved batch 35 of 94\n","Saved batch 36 of 94\n","Saved batch 37 of 94\n","Saved batch 38 of 94\n","Saved batch 39 of 94\n","Saved batch 40 of 94\n","Saved batch 41 of 94\n","Saved batch 42 of 94\n","Saved batch 43 of 94\n","Saved batch 44 of 94\n","Saved batch 45 of 94\n","Saved batch 46 of 94\n","Saved batch 47 of 94\n","Saved batch 48 of 94\n","Saved batch 49 of 94\n","Saved batch 50 of 94\n","Saved batch 51 of 94\n","Saved batch 52 of 94\n","Saved batch 53 of 94\n","Saved batch 54 of 94\n","Saved batch 55 of 94\n","Saved batch 56 of 94\n","Saved batch 57 of 94\n","Saved batch 58 of 94\n","Saved batch 59 of 94\n","Saved batch 60 of 94\n","Saved batch 61 of 94\n","Saved batch 62 of 94\n","Saved batch 63 of 94\n","Saved batch 64 of 94\n","Saved batch 65 of 94\n","Saved batch 66 of 94\n","Saved batch 67 of 94\n","Saved batch 68 of 94\n","Saved batch 69 of 94\n","Saved batch 70 of 94\n","Saved batch 71 of 94\n","Saved batch 72 of 94\n","Saved batch 73 of 94\n","Saved batch 74 of 94\n","Saved batch 75 of 94\n","Saved batch 76 of 94\n","Saved batch 77 of 94\n","Saved batch 78 of 94\n","Saved batch 79 of 94\n","Saved batch 80 of 94\n","Saved batch 81 of 94\n","Saved batch 82 of 94\n","Saved batch 83 of 94\n","Saved batch 84 of 94\n","Saved batch 85 of 94\n","Saved batch 86 of 94\n","Saved batch 87 of 94\n","Saved batch 88 of 94\n","Saved batch 89 of 94\n","Saved batch 90 of 94\n","Saved batch 91 of 94\n","Saved batch 92 of 94\n","Saved batch 93 of 94\n","Saved batch 94 of 94\n","\n","Dealing with Test now.\n","Saved batch 1 of 24\n","Saved batch 2 of 24\n","Saved batch 3 of 24\n","Saved batch 4 of 24\n","Saved batch 5 of 24\n","Saved batch 6 of 24\n","Saved batch 7 of 24\n","Saved batch 8 of 24\n","Saved batch 9 of 24\n","Saved batch 10 of 24\n","Saved batch 11 of 24\n","Saved batch 12 of 24\n","Saved batch 13 of 24\n","Saved batch 14 of 24\n","Saved batch 15 of 24\n","Saved batch 16 of 24\n","Saved batch 17 of 24\n","Saved batch 18 of 24\n","Saved batch 19 of 24\n","Saved batch 20 of 24\n","Saved batch 21 of 24\n","Saved batch 22 of 24\n","Saved batch 23 of 24\n","Saved batch 24 of 24\n","\n","Dealing with Test_Ignored now.\n","Saved batch 1 of 7\n","Saved batch 2 of 7\n","Saved batch 3 of 7\n","Saved batch 4 of 7\n","Saved batch 5 of 7\n","Saved batch 6 of 7\n","Saved batch 7 of 7\n","\n","Finished saving batches.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w8bejtKmJ3rm"},"source":["# Validation Procedures (Optional)"]},{"cell_type":"markdown","metadata":{"id":"yqHHQs3txiSZ"},"source":["## Patch Comparison"]},{"cell_type":"code","metadata":{"id":"rB-_GH6vxphV","cellView":"form","executionInfo":{"status":"ok","timestamp":1604004658618,"user_tz":-120,"elapsed":21443,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"ebf34c6e-d728-45e2-ef25-852f292a5a37","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11p6PRoO3P_ujnIXOTM6XpOBwdW10V4in"}},"source":["#@title Compare Patches\n","def get_height_width(odata):\n","  # Get footprint for interpolation\n","  footprint = odata['footprint'][9:-2].split(\",\")\n","  top_left_lat     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[1])\n","  top_left_lon     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[0])\n","  top_right_lat    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[1])\n","  top_right_lon    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[0])\n","  bottom_right_lat = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[1])\n","  bottom_right_lon = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[0])\n","  bottom_left_lat  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[1])\n","  bottom_left_lon  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[0])\n","  \n","  # Get coordinate pairs for each corner\n","  top_left     = (top_left_lat, top_left_lon)\n","  top_right    = (top_right_lat, top_right_lon)\n","  bottom_left  = (bottom_left_lat, bottom_left_lon)\n","  bottom_right = (bottom_right_lat, bottom_right_lon)\n","  \n","  # Assume the image is rectangular, and calculate the dimensions\n","  height = (geoDist.distance(top_left, bottom_left).km + geoDist.distance(top_right, bottom_right).km) / 2.0\n","  width  = (geoDist.distance(top_left, top_right).km + geoDist.distance(bottom_left, bottom_right).km) / 2.0\n","  \n","  return (height, width)\n","\n","# Create directory for Validation data\n","os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if validations_folder not in os.listdir(): os.mkdir(validations_folder)\n","\n","# Flag for everything being ready to make the comparison\n","ok_flag = True\n","\n","# Load list of sentinel images\n","os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","if \"Successful_Downloads.txt\" in os.listdir():\n","  with open(\"Successful_Downloads.txt\") as f:\n","    successful_downloads = f.read().splitlines()\n","else:\n","  print(\"No Image Downloads!\")\n","  ok_flag = False\n","\n","# Load list of labels completed\n","os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","if \"Successful_Labels.txt\" in os.listdir():\n","  with open(\"Successful_Labels.txt\") as f:\n","    successful_labels = f.read().splitlines()\n","else:\n","  print(\"No Labels Generated!\")\n","  ok_flag = False\n","\n","# Check that every image has a label\n","if ok_flag:\n","  for filename in successful_downloads:\n","    if filename not in successful_labels:\n","      ok_flag = False\n","      break\n","\n","# Make the comparison if everyhting is ok so far\n","if ok_flag:\n","  # Read in the list of ids which have already been processed\n","  os.chdir(os.path.join(dataset_location_path, dataset_name, validations_folder))\n","  already_processed = []\n","  if \"Patch_Label_Validations.txt\" in os.listdir():\n","    with open(\"Patch_Label_Validations.txt\", 'r') as f:\n","      already_processed = f.read().splitlines()\n","  \n","  # Only process ids which have not yet been processed\n","  needs_processing = []\n","  os.chdir(os.path.join(dataset_location_path, dataset_name))\n","  if \"Samples.txt\" in os.listdir():\n","    with open(\"Samples.txt\", 'r') as f:\n","      successful_downloads = f.read().splitlines()\n","  for filename in successful_downloads:\n","    if filename not in already_processed:\n","      needs_processing.append(filename)\n","\n","  # Process each id\n","  for filename in needs_processing[:20]:\n","    # Get date information\n","    y = filename.split(\"_\")[0]\n","    m = filename.split(\"_\")[1]\n","    d = filename.split(\"_\")[2]\n","    uuid = filename.split(\"_\")[3]\n","\n","    # Load the image\n","    os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n","    # full_image  = cv2.cvtColor(cv2.imread(sentinel_1_quicklook_name.format(y, m, d, uuid)), cv2.COLOR_BGR2RGB)\n","    small_image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(y, m, d, uuid)), cv2.COLOR_BGR2RGB)\n","\n","    # Load the labels\n","    os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n","    concentration = cv2.imread(concentration_name.format(y, m, d, uuid), cv2.IMREAD_GRAYSCALE)\n","    uncertainty   = cv2.imread(uncertainty_name.format(y, m, d, uuid), cv2.IMREAD_GRAYSCALE)\n","\n","    # Plot\n","    plt.gcf().set_size_inches(20,5)\n","    plt.suptitle(filename)\n","    # plt.subplot(1, 4, 1)\n","    # plt.imshow(full_image)\n","    # plt.title(\"Original SAR Image\")\n","    # plt.axis('off')\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(small_image)\n","    plt.title(\"Resampled SAR Image\")\n","    plt.axis('off')\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(concentration, cmap='hot', vmin=0, vmax=100)\n","    plt.title(\"Concentration Patch Label\")\n","    plt.axis('off')\n","    plt.colorbar()\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(uncertainty, cmap='hot', vmin=0, vmax=100)\n","    plt.title(\"Uncertainty Patch Label\")\n","    plt.axis('off')\n","    plt.colorbar()\n","\n","    # Save to Drive\n","    os.chdir(os.path.join(dataset_location_path, dataset_name, validations_folder))\n","    if not os.path.exists(\"Patch Label Validation/{}/{}/{}\".format(y, m, d)): os.makedirs(\"Patch Label Validation/{}/{}/{}\".format(y, m, d))\n","    plt.savefig(label_comparison_name.format(y, m, d, uuid), bbox_inches=0)\n","    # clear_output()\n","    plt.show()\n","\n","    odata = search_api.get_product_odata(uuid, full=True)\n","    size = get_height_width(odata)\n","    print(\"Done saving file {} of {}. Pixel Spacing (rg x az): {:.2f}km x {:.2f}km\".format(needs_processing.index(filename) + 1, len(needs_processing), size[1] / image_dimension, size[0] / image_dimension))\n","\n","    # Save the id to the text file for progress tracking\n","    with open(\"Patch_Label_Validations.txt\", 'a+') as f:\n","      f.write(filename + \"\\n\")\n","else:\n","  print(\"'ok_flag' tripped. Check that all previous steps have been completed correctly.\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"0IBNvazrxpsV"},"source":["### Delete all patch comparisons - Be careful!"]},{"cell_type":"code","metadata":{"id":"LfuTmKdQxp0M","executionInfo":{"status":"ok","timestamp":1603743720795,"user_tz":-120,"elapsed":52144,"user":{"displayName":"Muneeb Bray","photoUrl":"","userId":"06743350390058404227"}},"outputId":"e9644657-dd14-4b20-dabc-89139fdc1469","colab":{"base_uri":"https://localhost:8080/"}},"source":["os.chdir(os.path.join(dataset_location_path, dataset_name))\n","if validations_folder in os.listdir():\n","  os.chdir(validations_folder)\n","\n","  if \"Patch_Label_Validations.txt\" in os.listdir(): os.remove(\"Patch_Label_Validations.txt\")\n","\n","  if \"Patch Label Validation\" in os.listdir():\n","    for y in os.listdir(\"Patch Label Validation\"):\n","      for m in os.listdir(os.path.join(\"Patch Label Validation\", y)):\n","        for d in os.listdir(os.path.join(\"Patch Label Validation\", y, m)):\n","          for f in os.listdir(os.path.join(\"Patch Label Validation\", y, m, d)):\n","            os.remove(os.path.join(\"Patch Label Validation\", y, m, d, f))\n","          os.rmdir(os.path.join(\"Patch Label Validation\", y, m, d))\n","        os.rmdir(os.path.join(\"Patch Label Validation\", y, m))\n","      os.rmdir(os.path.join(\"Patch Label Validation\", y))\n","    os.rmdir(\"Patch Label Validation\")\n","\n","  if len(os.listdir()) == 0:\n","    os.chdir(\"..\")\n","    os.rmdir(validations_folder)\n","\n","  print(\"All files should have been deleted.\")\n","else:\n","  print(\"No validation data to delete.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["All files should have been deleted.\n"],"name":"stdout"}]}]}